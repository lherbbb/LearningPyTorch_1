{
 "cells": [
  {
   "metadata": {},
   "cell_type": "raw",
   "source": "#",
   "id": "49d90f0088f1322"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Chapter 05. Going Modular\n",
    "Turning notebooks into scripts\n"
   ],
   "id": "406bce7828a7ba3e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 0. Cell vs. script mode\n",
    "1. Cell mode is a notebook running normally each cell is either code or markdown\n",
    "2. Script mode is very similiar to a cell mode notebook, except many of the code cells are turned into Python scripts"
   ],
   "id": "6df1153004280195"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 1. Get data",
   "id": "9974190d5f79a1a8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-01T21:20:14.326042Z",
     "start_time": "2025-08-01T21:20:13.262925Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import requests\n",
    "import zipfile\n",
    "from pathlib import Path\n",
    "\n",
    "# Setup path to data folder\n",
    "data_path = Path(\"../data/\")\n",
    "image_path = data_path / \"pizza_steak_sushi\"\n",
    "\n",
    "# If image folder doesn't exist, download and prepare it...\n",
    "if image_path.is_dir():\n",
    "    print(f\"{image_path} directory exists.\")\n",
    "else:\n",
    "    print(f\"Did not find {image_path} directory, creating one...\")\n",
    "    image_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Download pizza, steak, sushi data\n",
    "with open(data_path / \"pizza_steak_sushi.zip\", \"wb\") as f:\n",
    "    request = requests.get(\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi.zip\")\n",
    "    print(\"Downloading pizza, steak, sushi data...\")\n",
    "    f.write(request.content)\n",
    "\n",
    "# Unzip pizza, steak, sushi data\n",
    "with zipfile.ZipFile(data_path / \"pizza_steak_sushi.zip\", \"r\") as zip_ref:\n",
    "    print(\"Unzipping pizza, steak, sushi data...\")\n",
    "    zip_ref.extractall(image_path)\n",
    "\n",
    "# Remove zip file\n",
    "os.remove(data_path / \"pizza_steak_sushi.zip\")"
   ],
   "id": "2e404ca0e9ea9e43",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/pizza_steak_sushi directory exists.\n",
      "Downloading pizza, steak, sushi data...\n",
      "Unzipping pizza, steak, sushi data...\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 2. Create Datasets and Dataloaders\n",
    "Once the data is ready we can convert 'Dataset' and 'Dataloader'ninto a new function 'create_dataloaders()'\n"
   ],
   "id": "54073e89c59b8cac"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-01T21:40:02.566903Z",
     "start_time": "2025-08-01T21:40:02.562252Z"
    }
   },
   "cell_type": "code",
   "source": [
    "%%writefile going_modular/data_setup.py\n",
    "import os\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "NUM_WORKERS = os.cpu_count()\n",
    "\n",
    "def create_dataloaders(\n",
    "    train_dir: str,\n",
    "    test_dir: str,\n",
    "    transform: transforms.Compose,\n",
    "    batch_size: int,\n",
    "    num_workers: int=NUM_WORKERS\n",
    "):\n",
    "\n",
    "  # Use ImageFolder to create dataset(s)\n",
    "  train_data = datasets.ImageFolder(train_dir, transform=transform)\n",
    "  test_data = datasets.ImageFolder(test_dir, transform=transform)\n",
    "\n",
    "  # Get class names\n",
    "  class_names = train_data.classes\n",
    "\n",
    "  # Turn images into data loaders\n",
    "  train_dataloader = DataLoader(\n",
    "      train_data,\n",
    "      batch_size=batch_size,\n",
    "      shuffle=True,\n",
    "      num_workers=num_workers,\n",
    "      pin_memory=True,\n",
    "  )\n",
    "  test_dataloader = DataLoader(\n",
    "      test_data,\n",
    "      batch_size=batch_size,\n",
    "      shuffle=False, # don't need to shuffle test data\n",
    "      num_workers=num_workers,\n",
    "      pin_memory=True,\n",
    "  )\n",
    "\n",
    "  return train_dataloader, test_dataloader, class_names"
   ],
   "id": "973263030d997054",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting going_modular/data_setup.py\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-01T21:40:23.900519Z",
     "start_time": "2025-08-01T21:40:17.435616Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Import data_setup.py\n",
    "from going_modular import data_setup\n",
    "\n",
    "\n",
    "# Create train/test dataloader and get class names as a list\n",
    "train_dataloader, test_dataloader, class_names = data_setup.create_dataloaders() # <- Put info here\n"
   ],
   "id": "586577dae73ce831",
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "create_dataloaders() missing 3 required positional arguments: 'test_dir', 'transform', and 'batch_size'",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mTypeError\u001B[39m                                 Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[10]\u001B[39m\u001B[32m, line 5\u001B[39m\n\u001B[32m      2\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mgoing_modular\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m data_setup\n\u001B[32m      4\u001B[39m \u001B[38;5;66;03m# Create train/test dataloader and get class names as a list\u001B[39;00m\n\u001B[32m----> \u001B[39m\u001B[32m5\u001B[39m train_dataloader, test_dataloader, class_names = \u001B[43mdata_setup\u001B[49m\u001B[43m.\u001B[49m\u001B[43mcreate_dataloaders\u001B[49m\u001B[43m(\u001B[49m\u001B[43m.\u001B[49m\u001B[43m.\u001B[49m\u001B[43m.\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[31mTypeError\u001B[39m: create_dataloaders() missing 3 required positional arguments: 'test_dir', 'transform', and 'batch_size'"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "44b1ea66a638d5ef"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 3. Making a model ('model_builder.py')\n",
   "id": "2619dd47a8b56dcc"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-01T21:53:50.741061Z",
     "start_time": "2025-08-01T21:53:50.734105Z"
    }
   },
   "cell_type": "code",
   "source": [
    "%%writefile going_modular/model_builder.py\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class TinyVGG(nn.Module):\n",
    "  def __init__(self, input_shape: int, hidden_units: int, output_shape: int) -> None:\n",
    "      super().__init__()\n",
    "      self.conv_block_1 = nn.Sequential(\n",
    "          nn.Conv2d(in_channels=input_shape,\n",
    "                    out_channels=hidden_units,\n",
    "                    kernel_size=3,\n",
    "                    stride=1,\n",
    "                    padding=0),\n",
    "          nn.ReLU(),\n",
    "          nn.Conv2d(in_channels=hidden_units,\n",
    "                    out_channels=hidden_units,\n",
    "                    kernel_size=3,\n",
    "                    stride=1,\n",
    "                    padding=0),\n",
    "          nn.ReLU(),\n",
    "          nn.MaxPool2d(kernel_size=2,\n",
    "                        stride=2)\n",
    "      )\n",
    "      self.conv_block_2 = nn.Sequential(\n",
    "          nn.Conv2d(hidden_units, hidden_units, kernel_size=3, padding=0),\n",
    "          nn.ReLU(),\n",
    "          nn.Conv2d(hidden_units, hidden_units, kernel_size=3, padding=0),\n",
    "          nn.ReLU(),\n",
    "          nn.MaxPool2d(2)\n",
    "      )\n",
    "      self.classifier = nn.Sequential(\n",
    "          nn.Flatten(),\n",
    "          # Where did this in_features shape come from?\n",
    "          # It's because each layer of our network compresses and changes the shape of our inputs data.\n",
    "          nn.Linear(in_features=hidden_units*13*13,\n",
    "                    out_features=output_shape)\n",
    "      )\n",
    "\n",
    "  def forward(self, x: torch.Tensor):\n",
    "      x = self.conv_block_1(x)\n",
    "      x = self.conv_block_2(x)\n",
    "      x = self.classifier(x)\n",
    "      return x\n",
    "      # return self.classifier(self.conv_block_2(self.conv_block_1(x))) # <- leverage the benefits of operator fusion"
   ],
   "id": "48bd9e189af4dde1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing going_modular/model_builder.py\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-01T21:54:40.983556Z",
     "start_time": "2025-08-01T21:54:40.933863Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "# Import model_builder.py\n",
    "from going_modular import model_builder\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device = \"mps\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Instantiate an instance of the model from the \"model_builder.py\" script\n",
    "torch.manual_seed(42)\n",
    "model = model_builder.TinyVGG(input_shape=3,\n",
    "                              hidden_units=10,\n",
    "                              output_shape=len(class_names)).to(device)"
   ],
   "id": "1c40849c5071c55f",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'class_names' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mNameError\u001B[39m                                 Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[12]\u001B[39m\u001B[32m, line 11\u001B[39m\n\u001B[32m      7\u001B[39m \u001B[38;5;66;03m# Instantiate an instance of the model from the \"model_builder.py\" script\u001B[39;00m\n\u001B[32m      8\u001B[39m torch.manual_seed(\u001B[32m42\u001B[39m)\n\u001B[32m      9\u001B[39m model = model_builder.TinyVGG(input_shape=\u001B[32m3\u001B[39m,\n\u001B[32m     10\u001B[39m                               hidden_units=\u001B[32m10\u001B[39m, \n\u001B[32m---> \u001B[39m\u001B[32m11\u001B[39m                               output_shape=\u001B[38;5;28mlen\u001B[39m(\u001B[43mclass_names\u001B[49m)).to(device)\n",
      "\u001B[31mNameError\u001B[39m: name 'class_names' is not defined"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "e3603c8dfb45819e"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
